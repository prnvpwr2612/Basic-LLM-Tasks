{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-03T20:05:16.837827Z",
     "iopub.status.busy": "2025-09-03T20:05:16.837524Z",
     "iopub.status.idle": "2025-09-03T20:06:50.978333Z",
     "shell.execute_reply": "2025-09-03T20:06:50.977336Z",
     "shell.execute_reply.started": "2025-09-03T20:05:16.837799Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.0/444.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mğŸ“„ PDF loaded successfully! Extracted 62974 characters. ğŸŠ\n",
      "Checkpoint: Reached PDF text extraction. Confirm to continue?\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain pypdf2 sentence-transformers langchain-community langchain-huggingface transformers -q\n",
    "\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "os.environ['PINECONE_API_KEY'] = os.getenv('PINECONE_API_KEY', 'Your-Key!')\n",
    "\n",
    "try:\n",
    "    loader = PyPDFLoader('/kaggle/input/llm-experiments-data/Drag-Drop LLMs.pdf')\n",
    "    documents = loader.load()\n",
    "    raw_text = ' '.join([doc.page_content for doc in documents])\n",
    "    print(f\"ğŸ“„ PDF loaded successfully! Extracted {len(raw_text)} characters. ğŸŠ\")\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš¨ Error: {str(e)} | Resolution: â­â­â­â­â­ Ensure 'sample.pdf' exists in current directory or check file permissions. Fallback: Use alternative loader.\")\n",
    "    raise\n",
    "\n",
    "print(\"Checkpoint: Reached PDF text extraction. Confirm to continue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:06:50.983094Z",
     "iopub.status.busy": "2025-09-03T20:06:50.982845Z",
     "iopub.status.idle": "2025-09-03T20:06:51.002538Z",
     "shell.execute_reply": "2025-09-03T20:06:51.001734Z",
     "shell.execute_reply.started": "2025-09-03T20:06:50.983068Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Split into 138 chunks! Ready for embedding. ğŸš€\n",
      "Checkpoint: Reached text chunking. Confirm to continue?\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "try:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_text(raw_text)\n",
    "    print(f\"âœ‚ï¸ Split into {len(chunks)} chunks! Ready for embedding. ğŸš€\")\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš¨ Error: {str(e)} | Resolution: â­â­â­â­â­ Retry with smaller chunk size or check text input. Fallback: Use simple string split.\")\n",
    "    raise\n",
    "\n",
    "# Checkpoint\n",
    "print(\"Checkpoint: Reached text chunking. Confirm to continue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:06:51.003768Z",
     "iopub.status.busy": "2025-09-03T20:06:51.003466Z",
     "iopub.status.idle": "2025-09-03T20:07:39.212266Z",
     "shell.execute_reply": "2025-09-03T20:07:39.211356Z",
     "shell.execute_reply.started": "2025-09-03T20:06:51.003741Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-03 20:07:06.461837: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756930026.682079      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756930026.750705      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7936ed444c49f79bffded72b3dea60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ea120efeda45498070bcf69c2dfa3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b03e5e98d34400b8dbfce258887f24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d69833a6c2942c3a11e98ad5c6997e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d837fe021fd747f7868ba2fb10d0d1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4d692a5d424cba9b779e6602bffd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f99978dcd74c10a6d1fe8376f9fff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d4934014c7466f9087e65cae00f4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f154eea35b104c2c99a6d0ee48af7b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4f560f9dcc4897ad47bcd54a5d3c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90c29460fbe4164bc44e4d34e8ffe1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Generated embeddings for 138 chunks. Dimensions: 384. ğŸŒŸ\n",
      "Checkpoint: Reached embedding generation. Confirm to continue?\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(chunks, show_progress_bar=False)\n",
    "    embedding_dim = embeddings.shape[1]\n",
    "    print(f\"ğŸ”¢ Generated embeddings for {len(chunks)} chunks. Dimensions: {embedding_dim}. ğŸŒŸ\")\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš¨ Error: {str(e)} | Resolution: â­â­â­â­â­ Retry once or fallback to smaller batch size. Ensure model is downloaded.\")\n",
    "    # Retry logic\n",
    "    try:\n",
    "        embeddings = [model.encode(chunk) for chunk in chunks]\n",
    "        print(\"ğŸ”„ Retry successful! Embeddings generated. ğŸ”„\")\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "# Checkpoint\n",
    "print(\"Checkpoint: Reached embedding generation. Confirm to continue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:08:14.255937Z",
     "iopub.status.busy": "2025-09-03T20:08:14.255554Z",
     "iopub.status.idle": "2025-09-03T20:08:15.968562Z",
     "shell.execute_reply": "2025-09-03T20:08:15.967362Z",
     "shell.execute_reply.started": "2025-09-03T20:08:14.255910Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping pinecone-client as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y --q pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:08:36.498545Z",
     "iopub.status.busy": "2025-09-03T20:08:36.498214Z",
     "iopub.status.idle": "2025-09-03T20:08:41.682814Z",
     "shell.execute_reply": "2025-09-03T20:08:41.681732Z",
     "shell.execute_reply.started": "2025-09-03T20:08:36.498520Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m259.3/259.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --q -U pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:08:50.837948Z",
     "iopub.status.busy": "2025-09-03T20:08:50.837572Z",
     "iopub.status.idle": "2025-09-03T20:09:00.375421Z",
     "shell.execute_reply": "2025-09-03T20:09:00.374371Z",
     "shell.execute_reply.started": "2025-09-03T20:08:50.837918Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—„ï¸ Pinecone index created and populated! Upserted 138 vectors. ğŸ“ˆ\n",
      "Checkpoint: Reached Pinecone population. Confirm to continue?\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "try:\n",
    "    pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
    "    index_name = 'pdf-query-index'\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(name=index_name, dimension=embedding_dim, metric='cosine', spec=ServerlessSpec(cloud='aws', region='us-east-1'))\n",
    "    index = pc.Index(index_name)\n",
    "    \n",
    "    vectors = [(str(i), embeddings[i].tolist(), {'text': chunks[i], 'source': 'sample.pdf'}) for i in range(len(chunks))]\n",
    "    index.upsert(vectors=vectors)\n",
    "    print(f\"ğŸ—„ï¸ Pinecone index created and populated! Upserted {len(vectors)} vectors. ğŸ“ˆ\")\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš¨ Error: {str(e)} | Resolution: â­â­â­â­â­ Check API key and network. Fallback: Delete and recreate index.\")\n",
    "    raise\n",
    "\n",
    "# Checkpoint\n",
    "print(\"Checkpoint: Reached Pinecone population. Confirm to continue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:09:29.345360Z",
     "iopub.status.busy": "2025-09-03T20:09:29.343284Z",
     "iopub.status.idle": "2025-09-03T20:09:29.400355Z",
     "shell.execute_reply": "2025-09-03T20:09:29.399323Z",
     "shell.execute_reply.started": "2025-09-03T20:09:29.345328Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fd186e059f4ea1b1e6b6332667d7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ Query embedded: 'Summarize the document.' â†’ Vector ready. ğŸ”\n",
      "Checkpoint: Reached query embedding. Confirm to continue?\n"
     ]
    }
   ],
   "source": [
    "query = \"Summarize the document.\"\n",
    "\n",
    "try:\n",
    "    query_embedding = model.encode(query).tolist()\n",
    "    print(f\"â“ Query embedded: '{query}' â†’ Vector ready. ğŸ”\")\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš¨ Error: {str(e)} | Resolution: â­â­â­â­â­ Ensure model is loaded. Fallback: Reload model.\")\n",
    "    raise\n",
    "\n",
    "# Checkpoint\n",
    "print(\"Checkpoint: Reached query embedding. Confirm to continue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:09:44.062777Z",
     "iopub.status.busy": "2025-09-03T20:09:44.061610Z",
     "iopub.status.idle": "2025-09-03T20:09:44.152375Z",
     "shell.execute_reply": "2025-09-03T20:09:44.151338Z",
     "shell.execute_reply.started": "2025-09-03T20:09:44.062733Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Found 3 similar chunks with scores: [0.299086124, 0.285904914, 0.274663955]! ğŸ“Š\n",
      "â±ï¸ Search latency: 0.08s\n",
      "Checkpoint: Reached Pinecone search. Confirm to continue?\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    search_results = index.query(vector=query_embedding, top_k=3, include_metadata=True)\n",
    "    retrieved_chunks = [match['metadata']['text'] for match in search_results['matches']]\n",
    "    scores = [match['score'] for match in search_results['matches']]\n",
    "    print(f\"ğŸ” Found {len(retrieved_chunks)} similar chunks with scores: {scores}! ğŸ“Š\")\n",
    "    latency = time.time() - start_time\n",
    "    print(f\"â±ï¸ Search latency: {latency:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš¨ Error: {str(e)} | Resolution: â­â­â­â­â­ Verify index exists. Fallback: Reinitialize index.\")\n",
    "    raise\n",
    "\n",
    "# Checkpoint\n",
    "print(\"Checkpoint: Reached Pinecone search. Confirm to continue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:13:34.432847Z",
     "iopub.status.busy": "2025-09-03T20:13:34.431139Z",
     "iopub.status.idle": "2025-09-03T20:15:36.013841Z",
     "shell.execute_reply": "2025-09-03T20:15:36.012943Z",
     "shell.execute_reply.started": "2025-09-03T20:13:34.432806Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c540bff590fc435589f149b6d39b68f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29aacab3268b4c22a5a759bbecd7bc71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02cfff394c0471ab13cfcafe1c13fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0749632a500f4b46ada1afd52f0bcf7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fb914bde6c4a508641259a802b379b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7856296a68f84098ae344df366347cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764e2a37a817420dad11bdb6182245f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd902dee082940e4ac550def4b3a51ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd6e8b4bbd1404c9aa7af924409b95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2e82bedcf94f57956cd25eb03bee7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5571b9a732884987b7f4e73702c39f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48d56f2fb3241d4930a8676d6cdccfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/tmp/ipykernel_36/3312365660.py:14: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n",
      "/tmp/ipykernel_36/3312365660.py:18: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run({'context': context, 'query': query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ Generated answer: Based on the following context: on learning distributions over the parameters, yet struggle to reconstruct original modelsâ€™ performance.\n",
      "With the development of diffusion models, Hyper-Representations... ğŸ¤ (Full response: Based on the following context: on learning distributions over the parameters, yet struggle to reconstruct original modelsâ€™ performance.\n",
      "With the development of diffusion models, Hyper-Representations [50, 51, 53] and p-diff [57], use\n",
      "the latent diffusion architecture to generate high-performing parameters. Armed with Mamba [20] and\n",
      "appropriate tokenization strategy, RPG [58] can generate 200M of parameters in minutes. Regarding\n",
      "conditional generation, COND P-DIFF [ 26], Tina [ 34] and ORAL [ 27] explores text-controlled [2] ajibawa 2023. ajibawa-2023/code-74k-sharegpt. Code-74k-ShareGPT, 2023.\n",
      "[3] Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh\n",
      "Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based\n",
      "formalisms. In NAACL, 2019.\n",
      "[4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\n",
      "Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923,\n",
      "2025. text-to-text transformer. JMLR, 21(140):1â€“67, 2020.\n",
      "[46] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\n",
      "networks. In EMNLP, 2019.\n",
      "[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer.\n",
      "High-resolution image synthesis with latent diffusion models. In CVPR, 2022.\n",
      "[48] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wad-\n",
      "Answer the query: Summarize the document.\n",
      "Make it concise and add a touch of humor if appropriate.\n",
      "\n",
      ")\n",
      "Checkpoint: Reached answer generation. Confirm to continue?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    from langchain_huggingface import HuggingFacePipeline\n",
    "    \n",
    "    pipe = pipeline(\"text-generation\", model=\"microsoft/Phi-3.5-mini-instruct\", max_new_tokens=100, device=-1)  # CPU\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    \n",
    "    prompt = PromptTemplate(template=\"Based on the following context: {context}\\nAnswer the query: {query}\\nMake it concise and add a touch of humor if appropriate.\")\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    \n",
    "    context = ' '.join(retrieved_chunks)\n",
    "    response = chain.run({'context': context, 'query': query})\n",
    "    print(f\"ğŸ’¬ Generated answer: {response[:200]}... ğŸ¤ (Full response: {response})\")\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš¨ Error: {str(e)} | Resolution: â­â­â­â­â­ Check LLM model download. Fallback: Use simpler prompt without LLM.\")\n",
    "    raise\n",
    "\n",
    "print(\"Checkpoint: Reached answer generation. Confirm to continue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T20:16:39.459014Z",
     "iopub.status.busy": "2025-09-03T20:16:39.458591Z",
     "iopub.status.idle": "2025-09-03T20:16:40.246319Z",
     "shell.execute_reply": "2025-09-03T20:16:40.245575Z",
     "shell.execute_reply.started": "2025-09-03T20:16:39.458988Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e335cc4adc45dfb1ed55caf7cf97fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Evaluation: Relevance 28.66% | Latency 0.08s | Suggestion: Tune chunk size to 300 for better speed if relevance drops below 80%. ğŸ†\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    retrieved_embeds = model.encode(retrieved_chunks)\n",
    "    similarities = cosine_similarity([query_embedding], retrieved_embeds)[0]\n",
    "    avg_relevance = np.mean(similarities) * 100\n",
    "    print(f\"ğŸ“Š Evaluation: Relevance {avg_relevance:.2f}% | Latency {latency:.2f}s | Suggestion: Tune chunk size to 300 for better speed if relevance drops below 80%. ğŸ†\")\n",
    "except Exception as e:\n",
    "    print(f\"ğŸš¨ Error: {str(e)} | Resolution: â­â­â­â­â­ Ensure embeddings are available. Fallback: Skip evaluation.\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8198022,
     "sourceId": 12953690,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
